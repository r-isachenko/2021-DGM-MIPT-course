{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YyxYA9wRwjgh"
   },
   "source": [
    "# Homework3: VAE & Normalizing flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16jlqZcZx6bj",
    "outputId": "aece1f85-ae8d-4118-870a-9cfc8b0443a6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "print('cuda is available:', USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZeXG1w4x7aG",
    "outputId": "934ef9b1-76e1-4905-ea50-6993db4ebe74"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeF5AMUIZFLg"
   },
   "source": [
    "## Task 1: VAE on CIFAR10 data (5pt)\n",
    "\n",
    "In this task you will implement VAE model for real image distribution (CIFAR10).\n",
    "\n",
    "Look carefully on the following helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4pfgRg3ZGMl"
   },
   "outputs": [],
   "source": [
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    train_data, test_data = data['train'], data['test']\n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "def show_samples(samples, title, nrow=10, figsize=(7, 7)):\n",
    "    samples = (torch.FloatTensor(samples) / 255).permute(0, 3, 1, 2)\n",
    "    grid_img = make_grid(samples, nrow=nrow)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    plt.imshow(grid_img.permute(1, 2, 0))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_data(data, title):\n",
    "    idxs = np.random.choice(len(data), replace=False, size=(100,))\n",
    "    images = train_data[idxs]\n",
    "    show_samples(images, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TObL_zwylGy"
   },
   "source": [
    "Here are the functions that we will you for training our model. Please, explore these functions carefully. You do not have to change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISQynkyLylSN"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, use_cuda, loss_key='total'):\n",
    "    model.train()\n",
    "  \n",
    "    stats = defaultdict(list)\n",
    "    for x in train_loader:\n",
    "        if use_cuda:\n",
    "            x = x.cuda()\n",
    "        losses = model.loss(x)\n",
    "        optimizer.zero_grad()\n",
    "        losses[loss_key].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        for k, v in losses.items():\n",
    "            stats[k].append(v.item())\n",
    "    return stats\n",
    "\n",
    "\n",
    "def eval_model(model, data_loader, use_cuda):\n",
    "    model.eval()\n",
    "    stats = defaultdict(float)\n",
    "    with torch.no_grad():\n",
    "        for x in data_loader:\n",
    "            if use_cuda:\n",
    "                x = x.cuda()\n",
    "            losses = model.loss(x)\n",
    "            for k, v in losses.items():\n",
    "                stats[k] += v.item() * x.shape[0]\n",
    "\n",
    "        for k in stats.keys():\n",
    "            stats[k] /= len(data_loader.dataset)\n",
    "    return stats\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs, lr, use_tqdm=False, use_cuda=False, loss_key='total_loss'):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = defaultdict(list)\n",
    "    test_losses = defaultdict(list)\n",
    "    forrange = tqdm(range(epochs)) if use_tqdm else range(epochs)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    for epoch in forrange:\n",
    "        model.train()\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, use_cuda, loss_key)\n",
    "        test_loss = eval_model(model, test_loader, use_cuda)\n",
    "\n",
    "        for k in train_loss.keys():\n",
    "            train_losses[k].extend(train_loss[k])\n",
    "            test_losses[k].append(test_loss[k])\n",
    "    return dict(train_losses), dict(test_losses)\n",
    "\n",
    "\n",
    "def plot_training_curves(train_losses, test_losses):\n",
    "    n_train = len(train_losses[list(train_losses.keys())[0]])\n",
    "    n_test = len(test_losses[list(train_losses.keys())[0]])\n",
    "    x_train = np.linspace(0, n_test - 1, n_train)\n",
    "    x_test = np.arange(n_test)\n",
    "\n",
    "    plt.figure()\n",
    "    for key, value in train_losses.items():\n",
    "        plt.plot(x_train, value, label=key + '_train')\n",
    "\n",
    "    for key, value in test_losses.items():\n",
    "        plt.plot(x_test, value, label=key + '_test')\n",
    "\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlabel('Epoch', fontsize=14)\n",
    "    plt.ylabel('Loss', fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw3tenScZKZl"
   },
   "source": [
    "Download CIFAR-10 data from here: https://drive.google.com/file/d/16j3nrJV821VOkkuRz7aYam8TyIXLnNme/view?usp=sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "id": "x29n3mA3ZGPw",
    "outputId": "45fe740c-74a5-4478-fa77-ba4f58fb6dc1"
   },
   "outputs": [],
   "source": [
    "# change the path to the generated data\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'MyDrive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "visualize_data(train_data, 'CIFAR10 samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YfgTRQ0ZOKB"
   },
   "source": [
    "Here the model specification will be almost the same (as in hw2) with the following differences:\n",
    "* Now our encoder and decoder will be convolutional.\n",
    "* We do not model covariance matrix in generative distribution (now it is identical). We will use means of the generative distribution as model samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmmgRCeB55ub"
   },
   "outputs": [],
   "source": [
    "def get_normal_KL(mean_1, log_std_1, mean_2=None, log_std_2=None):\n",
    "    \"\"\"\n",
    "        This function should return the value of KL(p1 || p2),\n",
    "        where p1 = Normal(mean_1, exp(log_std_1)), p2 = Normal(mean_2, exp(log_std_2)).\n",
    "        If mean_2 and log_std_2 are None values, we will use standart normal distribution.\n",
    "        Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    if mean_2 is None:\n",
    "        mean_2 = torch.zeros_like(mean_1)\n",
    "    if log_std_2 is None:\n",
    "        log_std_2 = torch.zeros_like(log_std_1)\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_KL():\n",
    "    assert np.isclose(get_normal_KL(torch.tensor(2), torch.tensor(3), torch.tensor(0), torch.tensor(0)).numpy(), 200.2144, rtol=1e-3)\n",
    "    assert np.isclose(get_normal_KL(torch.tensor(2), torch.tensor(3), torch.tensor(4), torch.tensor(5)).numpy(), 1.50925, rtol=1e-3)\n",
    "    assert np.allclose(get_normal_KL(torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))).numpy(), [49.2990, 1498.479], rtol=1e-3)\n",
    "\n",
    "test_KL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXtq8fYM55yD"
   },
   "outputs": [],
   "source": [
    "def get_normal_nll(x, mean, log_std):\n",
    "    \"\"\"\n",
    "        This function should return the negative log likelihood log p(x),\n",
    "        where p(x) = Normal(x | mean, exp(log_std)).\n",
    "        Note that we consider the case of diagonal covariance matrix.\n",
    "    \"\"\"\n",
    "    # ====\n",
    "    # your code\n",
    "    \n",
    "    # ====\n",
    "\n",
    "\n",
    "def test_NLL():\n",
    "    assert np.isclose(get_normal_nll(torch.tensor(2), torch.tensor(2), torch.tensor(3)).numpy(), 3.9189, rtol=1e-3)\n",
    "    assert np.isclose(get_normal_nll(torch.tensor(5), torch.tensor(-3), torch.tensor(6)).numpy(), 6.9191, rtol=1e-3)\n",
    "    assert np.allclose(get_normal_nll(torch.tensor((10, 10)), torch.tensor((2, 4)), torch.tensor((3, 5))).numpy(), np.array([3.9982, 5.9197]), rtol=1e-3)\n",
    "\n",
    "test_NLL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhR0oBcXZGTA"
   },
   "outputs": [],
   "source": [
    "class ConvEncoder(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # conv2d(32) -> relu -> conv(64) -> relu -> conv(128) -> relu -> conv(256) -> fc(2 * n_latent)\n",
    "        # but we encourage you to create your own architecture\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply convs\n",
    "        # 2) reshape the output to 2d matrix for last fc layer\n",
    "        # 3) apply fc layer\n",
    "\n",
    "        # ====\n",
    "        return mu, log_std\n",
    "        \n",
    "\n",
    "class ConvDecoder(nn.Module):\n",
    "    def __init__(self, n_latent, output_shape):\n",
    "        super().__init__()\n",
    "        self.n_latent = n_latent\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        self.base_size = (128, output_shape[1] // 8, output_shape[2] // 8)\n",
    "        # ====\n",
    "        # your code\n",
    "        # we suggest to use the following architecture\n",
    "        # fc -> conv2dtranspose(128) -> relu -> conv2dtranspose(64) -> relu \n",
    "        # -> conv2dtranspose(32) -> relu -> conv2dtranspose(3)\n",
    "        # but we encourage you to create your own architecture\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, z):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply fc layer\n",
    "        # 2) reshape the output to 4d tensor \n",
    "        # 3) apply conv layers\n",
    "\n",
    "        # ====\n",
    "        return out\n",
    "\n",
    "\n",
    "class ConvVAE(nn.Module):\n",
    "    def __init__(self, input_shape, n_latent):\n",
    "        super().__init__()\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.n_latent = n_latent\n",
    "        # ====\n",
    "        # your code\n",
    "        # define encoder with input size input_shape and output dim n_latent\n",
    "        # define decoder with input dim n_latent and output size input_shape\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def prior(self, n):\n",
    "        # ====\n",
    "        # your code\n",
    "        # return n samples from prior distribution (we use standart normal for prior)\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) apply encoder to get mu_z, log_std_z\n",
    "        # 2) apply reparametrization trick (use self.prior)\n",
    "        # 3) apply decoder to get mu_x (which corresponds to reconstructed x)\n",
    "\n",
    "        # ====\n",
    "        return mu_z, log_std_z, x_recon\n",
    "        \n",
    "    def loss(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) make forward step to get mu_z, log_std_z, x_recon\n",
    "        # 2) calculate recon_loss (use get_normal_nll)\n",
    "        # 3) calcucalte kl_loss (use get_normal_KL)\n",
    "\n",
    "        # ==== \n",
    "        return {\n",
    "            'elbo_loss': recon_loss + kl_loss, \n",
    "            'recon_loss': recon_loss,\n",
    "            'kl_loss': kl_loss\n",
    "        }\n",
    "\n",
    "    def sample(self, n):\n",
    "        with torch.no_grad():\n",
    "            # ====\n",
    "            # your code\n",
    "            # 1) generate prior samples\n",
    "            # 2) apply decoder\n",
    "\n",
    "            # ====\n",
    "            samples = torch.clamp(x_recon, -1, 1)\n",
    "        return samples.cpu().permute(0, 2, 3, 1).numpy() * 0.5 + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "MAtWnpnsZGWS",
    "outputId": "f03deea9-7b66-461f-9602-3262ee7054e2"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "N_LATENS = \n",
    "\n",
    "# ====\n",
    "\n",
    "# change the path to the data\n",
    "train_data, test_data = load_pickle(os.path.join('drive', 'MyDrive', 'DGM', 'homework_supplementary', 'cifar10.pkl'))\n",
    "\n",
    "train_data = (np.transpose(train_data, (0, 3, 1, 2)) / 255. * 2 - 1).astype('float32')\n",
    "test_data = (np.transpose(test_data, (0, 3, 1, 2)) / 255. * 2 - 1).astype('float32')\n",
    "\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE)\n",
    "\n",
    "model = ConvVAE((3, 32, 32), N_LATENS).cuda()\n",
    "train_losses, test_losses = train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    test_loader, \n",
    "    epochs=EPOCHS, \n",
    "    lr=LR, \n",
    "    loss_key='elbo_loss', \n",
    "    use_tqdm=True, \n",
    "    use_cuda=USE_CUDA, \n",
    ")\n",
    "for key, value in test_losses.items():\n",
    "    print('{}: {:.4f}'.format(key, value[-1]))\n",
    "plot_training_curves(train_losses, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "R-CoqopEZGaV",
    "outputId": "6d723d18-1abe-4ebe-aa82-c33dc6835b2c"
   },
   "outputs": [],
   "source": [
    "samples = model.sample(100) * 255.\n",
    "\n",
    "x = next(iter(test_loader))[:50].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    x_recon = torch.clamp(model.decoder(z), -1, 1)\n",
    "reconstructions = torch.stack((x, x_recon), dim=1).view(-1, 3, 32, 32) * 0.5 + 0.5\n",
    "reconstructions = reconstructions.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "x = next(iter(test_loader))[:20].cuda()\n",
    "with torch.no_grad():\n",
    "    z, _ = model.encoder(x)\n",
    "    z1, z2 = z.chunk(2, dim=0)\n",
    "    interps = [model.decoder(z1 * (1 - alpha) + z2 * alpha) for alpha in np.linspace(0, 1, 10)]\n",
    "    interps = torch.stack(interps, dim=1).view(-1, 3, 32, 32)\n",
    "    interps = torch.clamp(interps, -1, 1) * 0.5 + 0.5\n",
    "interps = interps.permute(0, 2, 3, 1).cpu().numpy() * 255\n",
    "\n",
    "samples, reconstructions, interps = samples.astype('float32'), reconstructions.astype('float32'), interps.astype('float32')\n",
    "\n",
    "show_samples(reconstructions, 'CIFAR10 reconstructions')\n",
    "show_samples(samples, 'CIFAR10 samples')\n",
    "show_samples(interps, 'CIFAR10 interpolation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nHF8HOV6wzPy",
    "tags": []
   },
   "source": [
    "## Task 2: Theory (3pt)\n",
    "\n",
    "At the lecture 5, we studied planar flows of the form:\n",
    "$$\n",
    "    \\mathbf{x} = g(\\mathbf{z}, \\boldsymbol{\\theta}) = \\mathbf{z} + \\mathbf{u} h(\\mathbf{w}^T\\mathbf{z} + b),\n",
    "$$\n",
    "where $\\mathbf{u} \\in \\mathbb{R}^m$,  $\\mathbf{w} \\in \\mathbb{R}^m$, $b \\in \\mathbb{R}$.\n",
    "\n",
    "There is a natural generalization of planar flows of the form:\n",
    "$$\n",
    "   \\mathbf{x} = g(\\mathbf{z}, \\boldsymbol{\\theta}) = \\mathbf{z} + \\mathbf{V} h(\\mathbf{W}^T\\mathbf{z} + \\mathbf{b}),\n",
    "$$\n",
    "where $\\mathbf{V} \\in \\mathbb{R}^{m \\times k}$,  $\\mathbf{W} \\in \\mathbb{R}^{m \\times k}$, $\\mathbf{b} \\in \\mathbb{R}^k$. Such a flow is called [Sylvester flow](https://arxiv.org/abs/1803.05649).\n",
    "* Prove a simplified version of the matrix-determinant lemma:\n",
    "$$\n",
    "    \\det (\\mathbf{I}_m + \\mathbf{V} \\mathbf{W}^T) = \\det (\\mathbf{I}_k + \\mathbf{W}^T \\mathbf{V}).\n",
    "$$\n",
    "* Calculate the determinant of the Jacobi matrix for the Sylvester flow transformation and apply the lemma proved in the previous paragraph to it.\n",
    "* In order to reduce the complexity of the calculation of the received determinants, the authors proposed to apply the QR-decomposition of the form:\n",
    "$$\n",
    "    \\mathbf{V} = \\mathbf{Q} \\mathbf{U}; \\quad \\mathbf{W} = \\mathbf{Q} \\mathbf{L},\n",
    "$$\n",
    "where $\\mathbf{Q} \\in \\mathbb{R}^{m \\times k}$ - orthogonal matrix($\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I}$), $\\mathbf{U} \\in \\mathbb{R}^{k \\times k}$ -- upper triangular matrix, $\\mathbf{L} \\in \\mathbb{R}^{k \\times k}$ - upper triangular matrix. Write out an expression for the Jacobian determinant using this decomposition.\n",
    "* Calculate and compare the difficulties for calculating the determinant of the Jacobi matrix before applying the lemma, after applying the lemma, and after applying the QR decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-GAM_JpruKC"
   },
   "source": [
    "```your solution```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqoNm0OqyBiy"
   },
   "source": [
    "## Task 3: RealNVP on 2d data (5pt)\n",
    "\n",
    "In this task you will implement RealNVP model on 2d moons dataset. \n",
    "\n",
    "The following function generates the data (do not change it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvRAjePZyHgC"
   },
   "outputs": [],
   "source": [
    "def generate_moons_data(count):\n",
    "    data, labels = make_moons(n_samples=count, noise=0.1)\n",
    "    data = data.astype('float32')\n",
    "    split = int(0.8 * count)\n",
    "    train_data, test_data = data[:split], data[split:]\n",
    "    train_labels, test_labels = labels[:split], labels[split:]\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "\n",
    "def visualize_2d_data(train_data, test_data, train_labels=None, test_labels=None):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.set_title('train', fontsize=16)\n",
    "    ax1.scatter(train_data[:, 0], train_data[:, 1], s=1, c=train_labels)\n",
    "    ax1.tick_params(labelsize=16)\n",
    "    ax2.set_title('test', fontsize=16)\n",
    "    ax2.scatter(test_data[:, 0], test_data[:, 1], s=1, c=test_labels)\n",
    "    ax2.tick_params(labelsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "8fVx4EVYyHjK",
    "outputId": "682dcfa5-825c-4d0b-9fae-bca748eccd21"
   },
   "outputs": [],
   "source": [
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "visualize_2d_data(train_data, test_data, train_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6W74_xcyLt5"
   },
   "source": [
    "See original paper for model description (https://arxiv.org/abs/1605.08803).\n",
    "\n",
    "The model is a sequence of affine coupling layers. Note that you have to permute the features that will left unchanged between different layers (change order of $\\\\mathbf{x}_1$ and $\\mathbf{2}$ in formulas below).\n",
    "\n",
    "Forward transform:\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\mathbf{y}_1 &= \\mathbf{x}_1; \\\\\n",
    "        \\mathbf{y}_2 &= \\mathbf{x}_2 \\odot \\exp (s(\\mathbf{x}_1)) + t(\\mathbf{x}_1).\n",
    "    \\end{cases} \n",
    "$$\n",
    "\n",
    "Inverse transform:\n",
    "$$\n",
    "    \\begin{cases}\n",
    "        \\mathbf{x}_1 &= \\mathbf{y}_1; \\\\\n",
    "        \\mathbf{x}_2 &= (\\mathbf{y}_2 - t(\\mathbf{y}_1)) \\odot \\exp ( - s(\\mathbf{y}_1)).\n",
    "    \\end{cases} \n",
    "$$\n",
    "\n",
    "Here $s(\\cdot)$ and $t(\\cdot)$ are outputs of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5Xe7lJRyHmW"
   },
   "outputs": [],
   "source": [
    "class FullyConnectedMLP(nn.Module):\n",
    "    def __init__(self, input_shape, hiddens, output_shape):\n",
    "        assert isinstance(hiddens, list)\n",
    "        super().__init__()\n",
    "        self.input_shape = (input_shape,)\n",
    "        self.output_shape = (output_shape,)\n",
    "        self.hiddens = hiddens\n",
    "\n",
    "        model = []\n",
    "\n",
    "        # ====\n",
    "        # your code \n",
    "        # Stack Dense layers with ReLU activation.\n",
    "        # Note that you do not have to add relu after the last dense layer\n",
    "\n",
    "        # ====\n",
    "        self.net = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply network that was defined in __init__ and return the output\n",
    "\n",
    "        # ====\n",
    "\n",
    "\n",
    "class AffineTransform(nn.Module):\n",
    "    def __init__(self, partition_type, n_hiddens):\n",
    "        assert isinstance(n_hiddens, list)\n",
    "        super().__init__()\n",
    "        self.mask = self.build_mask(partition_type=partition_type)\n",
    "        self.scale = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.scale_shift = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "        self.mlp = FullyConnectedMLP(input_shape=2, hiddens=n_hiddens, output_shape=2)\n",
    "\n",
    "    def build_mask(self, partition_type):\n",
    "        assert partition_type in {\"left\", \"right\"}\n",
    "        # ====\n",
    "        # your code\n",
    "        # mask is extremely simple\n",
    "        # it is a float tensor of two scalars (1.0 and 0.0)\n",
    "        # the partition_type defines the order of these two scalars\n",
    "\n",
    "        # ====\n",
    "\n",
    "    def forward(self, x, invert=False):\n",
    "        # ====\n",
    "        # your code\n",
    "        # you have to mask our input x\n",
    "        # repeat mask batch_size times and apply x on mask\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply mlp to masked input to get log_s and t \n",
    "\n",
    "        # ====\n",
    "\n",
    "        # this formula is described in Section 4.1 in original paper\n",
    "        # just left it unchanged\n",
    "        log_s = self.scale * torch.tanh(log_s) + self.scale_shift\n",
    "\n",
    "        # note that we invert mask here\n",
    "        t = t * (1.0 - mask)\n",
    "        log_s = log_s * (1.0 - mask)\n",
    "\n",
    "        # ====\n",
    "        # your code\n",
    "        # if invert=True use reverse transform, else use forward transform\n",
    "\n",
    "        # ====\n",
    "\n",
    "        # the output is transformed input \n",
    "        # and logarithm of jacobian which equals to log_s\n",
    "        return x, log_s\n",
    "\n",
    "\n",
    "class RealNVP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # base distribution is normal\n",
    "        self.prior = torch.distributions.Normal(torch.tensor(0.), torch.tensor(1.))\n",
    "        # ====\n",
    "        # your code\n",
    "        # apply sequence of AffineTransform with alternating partition_type\n",
    "        # 6 layers is sufficient (with 2 hidden layers in each affine layer)\n",
    "\n",
    "        # ====\n",
    "        \n",
    "    def forward(self, x, invert=False):\n",
    "        z = x\n",
    "        log_det = 0.0\n",
    "        transforms = self.transforms[::-1] if invert else self.transforms\n",
    "\n",
    "        for transform in transforms:\n",
    "            z, delta_log_det = transform(z, invert=invert)\n",
    "            log_det += delta_log_det\n",
    "\n",
    "        return z, log_det\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) make forward pass with right inverse flag\n",
    "        # 2) sum log_det with log of base distribution\n",
    "        # ====\n",
    "\n",
    "    def loss(self, x):\n",
    "        return {'nll_loss': -self.log_prob(x).mean()}\n",
    "\n",
    "    def sample(self, n):\n",
    "        # ====\n",
    "        # your code\n",
    "        # 1) sample from the prior\n",
    "        # 2) apply the forward pass with the right inverse flag\n",
    "        # 3) return only the first output of forward pass\n",
    "        \n",
    "        # ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUbz9MEeyQby"
   },
   "source": [
    "Here the helper functions for visualization. Look at them carefully and do not change them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hbA0USMCyOJQ"
   },
   "outputs": [],
   "source": [
    "def show_2d_latents(latents, labels=None, title='Latent Space'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if labels is None:\n",
    "        labels = 'green'\n",
    "    plt.scatter(latents[:, 0], latents[:, 1], s=1, c=labels)\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def show_2d_densities(densities, title='Densities'):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    dx, dy = 0.025, 0.025\n",
    "    x_lim = (-1.5, 2.5)\n",
    "    y_lim = (-1, 1.5)\n",
    "    y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                    slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.pcolor(x, y, densities.reshape([y.shape[0], y.shape[1]]))\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "iTsZH3rXyOME",
    "outputId": "993b0526-b323-4abd-a27f-f37b22258f03"
   },
   "outputs": [],
   "source": [
    "# ====\n",
    "# your code\n",
    "# choose these parameters\n",
    "\n",
    "BATCH_SIZE = \n",
    "EPOCHS = \n",
    "LR = \n",
    "# ====\n",
    "\n",
    "COUNT = 5000\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = generate_moons_data(COUNT)\n",
    "\n",
    "loader_args = dict(batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = data.DataLoader(train_data, **loader_args)\n",
    "test_loader = data.DataLoader(test_data, **loader_args)\n",
    "\n",
    "# model\n",
    "real_nvp = RealNVP().cuda()\n",
    "\n",
    "# train\n",
    "train_losses, test_losses = train_model(\n",
    "    real_nvp, train_loader, test_loader, epochs=EPOCHS, lr=LR, loss_key='nll_loss', use_cuda=USE_CUDA\n",
    ")\n",
    "\n",
    "# heatmap\n",
    "dx, dy = 0.025, 0.025\n",
    "x_lim = (-1.5, 2.5)\n",
    "y_lim = (-1, 1.5)\n",
    "y, x = np.mgrid[slice(y_lim[0], y_lim[1] + dy, dy),\n",
    "                slice(x_lim[0], x_lim[1] + dx, dx)]\n",
    "mesh_xs = torch.FloatTensor(np.stack([x, y], axis=2).reshape(-1, 2)).cuda()\n",
    "densities = np.exp(real_nvp.log_prob(mesh_xs).cpu().detach().numpy())\n",
    "\n",
    "# latents\n",
    "z = real_nvp(torch.FloatTensor(train_data).cuda())[0]\n",
    "latents = z.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "plot_training_curves(train_losses, test_losses)\n",
    "show_2d_densities(densities)\n",
    "show_2d_latents(latents, train_labels)\n",
    "\n",
    "x_samples = real_nvp.sample(4000).cpu().detach().numpy()\n",
    "show_2d_latents(x_samples)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
